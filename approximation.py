# -*- coding: utf-8 -*-
"""Untitled42.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-kJ0UNPCDOKhW4SnXesIOnW0ZGGxX3Fj
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
from matplotlib.patches import FancyArrow

# ------------------------- Agent -------------------------
class Agent:
    def __init__(self):
        self.start = None
        self.pos = None

    def set_position(self, start):
        self.start = tuple(start)

    def reset(self):
        self.pos = tuple(self.start)

    def move(self, action, height, width):
        r, c = self.pos
        if action == 0: r -= 1     # Up
        elif action == 1: c += 1   # Right
        elif action == 2: r += 1   # Down
        elif action == 3: c -= 1   # Left

        if 0 <= r < height and 0 <= c < width:
            self.pos = (r, c)
        return self.pos


# ------------------------- GridWorld Environment -------------------------
class GridWorld:
    def __init__(self, height=5, width=5, start=None, goal=None,
                 reward_goal=1, reward_step=-1, obstacles=None, obstacle_penalty=-100):
        self.height, self.width = height, width
        self.obstacles = set(tuple(o) for o in obstacles) if obstacles else set()

        self.start = tuple(start) if start else self._random_free_cell([])
        self.goals = [tuple(g) for g in goal] if goal else [self._random_free_cell([self.start] + list(self.obstacles))]

        for g in self.goals:
            assert g not in self.obstacles
        assert self.start not in self.obstacles

        self.agent = Agent()
        self.agent.set_position(self.start)

        self.reward_goal = reward_goal
        self.reward_step = reward_step
        self.obstacle_penalty = obstacle_penalty
        self._steps = 0
        self.trajectory = []

        self.cmap = colors.ListedColormap(["#f0f0f0", "#4285f4", "#34a853", "#202124"])
        self.norm = colors.BoundaryNorm([0, 2, 3, 4, 5], self.cmap.N)

    def _random_free_cell(self, exclude=[]):
        exclude_set = set(exclude)
        while True:
            cell = (np.random.randint(0, self.height), np.random.randint(0, self.width))
            if cell not in exclude_set and cell not in self.obstacles:
                return cell

    def set_random_goal(self):
        exclude = list(self.obstacles) + [self.agent.pos]
        self.goals = [self._random_free_cell(exclude=exclude)]

    def set_start_position(self, pos):
        assert pos not in self.obstacles and pos not in self.goals
        self.start = pos
        self.agent.set_position(pos)

    def reset(self, randomize_goal=False):
        if randomize_goal:
            self.set_random_goal()
        self.agent.reset()
        self._steps = 0
        self.trajectory = [self.agent.pos]
        return self._get_obs()

    def step(self, action):
        self._steps += 1
        new_pos = self.agent.move(action, self.height, self.width)
        self.trajectory.append(new_pos)
        if new_pos in self.goals:
            return self._get_obs(), self.reward_goal, True
        elif new_pos in self.obstacles:
            return self._get_obs(), self.obstacle_penalty, False
        else:
            return self._get_obs(), self.reward_step, False

    def _get_obs(self):
        grid = np.zeros((self.height, self.width), dtype=np.int8)
        for r, c in self.obstacles:
            grid[r, c] = 4
        for r, c in self.goals:
            grid[r, c] = 3
        ar, ac = self.agent.pos
        grid[ar, ac] = 2
        return grid

    def render(self, pause_time=1, show_trajectory=True, show_policy=False, agent=None):
        grid = self._get_obs()
        plt.clf()

        # Create figure with subplots if showing policy
        if show_policy and agent:
            fig = plt.gcf()
            fig.clear()
            ax1 = plt.subplot(1, 2, 1)
            ax2 = plt.subplot(1, 2, 2)

            # Left: Current state
            ax1.imshow(grid, cmap=self.cmap, norm=self.norm, extent=[0, self.width, self.height, 0])
            ax1.set_xticks(np.arange(0, self.width + 1, 1))
            ax1.set_yticks(np.arange(0, self.height + 1, 1))
            ax1.set_xticklabels([])
            ax1.set_yticklabels([])
            ax1.grid(color="gray", linewidth=1)
            ax1.set_title(f"Agent Position (Step: {self._steps})")

            # Draw trajectory
            if show_trajectory and len(self.trajectory) > 1:
                traj_array = np.array(self.trajectory)
                ax1.plot(traj_array[:, 1] + 0.5, traj_array[:, 0] + 0.5,
                        'r-', linewidth=2, alpha=0.6, marker='o', markersize=4)

            # Right: Policy arrows
            value_grid = np.zeros((self.height, self.width))
            for r in range(self.height):
                for c in range(self.width):
                    if (r, c) not in self.obstacles:
                        value_grid[r, c] = max(agent.q_value((r, c), a) for a in range(4))

            im = ax2.imshow(value_grid, cmap='RdYlGn', extent=[0, self.width, self.height, 0])
            ax2.set_xticks(np.arange(0, self.width + 1, 1))
            ax2.set_yticks(np.arange(0, self.height + 1, 1))
            ax2.set_xticklabels([])
            ax2.set_yticklabels([])
            ax2.grid(color="gray", linewidth=1)
            ax2.set_title("Learned Policy & Values")
            plt.colorbar(im, ax=ax2, label="Max Q-Value")

            # Draw policy arrows
            arrow_dirs = [(-0.3, 0), (0, 0.3), (0.3, 0), (0, -0.3)]  # Up, Right, Down, Left
            for r in range(self.height):
                for c in range(self.width):
                    if (r, c) not in self.obstacles and (r, c) not in self.goals:
                        best_action = agent.act((r, c))
                        dy, dx = arrow_dirs[best_action]
                        ax2.arrow(c + 0.5, r + 0.5, dx, dy,
                                 head_width=0.15, head_length=0.1,
                                 fc='white', ec='black', linewidth=1.5)

            # Mark obstacles and goals
            for r, c in self.obstacles:
                ax2.add_patch(plt.Rectangle((c, r), 1, 1, fill=True, color='black', alpha=0.3))
            for r, c in self.goals:
                ax2.scatter(c + 0.5, r + 0.5, s=300, c='gold', marker='*', edgecolors='black', linewidth=2)

        else:
            # Single plot
            plt.imshow(grid, cmap=self.cmap, norm=self.norm, extent=[0, self.width, self.height, 0])
            plt.xticks(np.arange(0, self.width + 1, 1), [])
            plt.yticks(np.arange(0, self.height + 1, 1), [])
            plt.grid(color="gray", linewidth=1.5)
            plt.title(f"GridWorld - Steps: {self._steps}", fontsize=14, fontweight='bold')

            # Draw trajectory
            if show_trajectory and len(self.trajectory) > 1:
                traj_array = np.array(self.trajectory)
                plt.plot(traj_array[:, 1] + 0.5, traj_array[:, 0] + 0.5,
                        'r-', linewidth=2.5, alpha=0.7, marker='o', markersize=5)

        plt.tight_layout()
        plt.pause(pause_time)


class LinearQAgent:
    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=1.0,
                 epsilon_min=0.01, epsilon_decay=0.995):
        self.env = env
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.num_features = env.height * env.width * 4
        self.w = np.zeros(self.num_features)

    def featurize(self, state, action):
        r, c = state
        feat = np.zeros(self.num_features)
        idx = (r * self.env.width + c) * 4 + action
        feat[idx] = 1.0
        return feat

    def q_value(self, state, action):
        return np.dot(self.w, self.featurize(state, action))

    def choose_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.choice(4)
        else:
            return np.argmax([self.q_value(state, a) for a in range(4)])

    def learn(self, num_episodes=1000, max_steps=100, random_start=False, moving_goal=False):
        rewards_history = []
        for ep in range(num_episodes):
            if random_start:
                while True:
                    rand_pos = (np.random.randint(0, self.env.height), np.random.randint(0, self.env.width))
                    if rand_pos not in self.env.obstacles and rand_pos not in self.env.goals:
                        self.env.set_start_position(rand_pos)
                        break
            if moving_goal:
                self.env.set_random_goal()

            self.env.reset()
            state = self.env.agent.pos
            done = False
            steps = 0
            total_reward = 0

            while not done and steps < max_steps:
                action = self.choose_action(state)
                phi = self.featurize(state, action)
                obs, reward, done = self.env.step(action)
                next_state = self.env.agent.pos
                total_reward += reward

                if done:
                    q_next = 0
                else:
                    q_next = max(self.q_value(next_state, a) for a in range(4))

                td_error = (reward + self.gamma * q_next) - self.q_value(state, action)
                self.w += self.alpha * td_error * phi
                state = next_state
                steps += 1

            rewards_history.append(total_reward)
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
            if (ep + 1) % 100 == 0:
                avg_reward = np.mean(rewards_history[-100:])
                print(f"Episode {ep+1}, Epsilon: {self.epsilon:.3f}, Avg Reward: {avg_reward:.2f}")

    def act(self, state):
        return np.argmax([self.q_value(state, a) for a in range(4)])

    def get_q_grid(self):
        q_grid = np.zeros((self.env.height, self.env.width, 4))
        for r in range(self.env.height):
            for c in range(self.env.width):
                for a in range(4):
                    q_grid[r, c, a] = self.q_value((r, c), a)
        return q_grid

if __name__ == "__main__":
    height, width = 6, 6
    start = (0, 0)
    goals = [(3, 3)]
    obstacles = [(1, 1), (3, 2)]

    env = GridWorld(height=height, width=width, start=start, goal=goals,
                    reward_goal=10, reward_step=-1, obstacles=obstacles)

    agent = LinearQAgent(env, gamma=0.95, alpha=0.2, epsilon=1.0,
                         epsilon_min=0.01, epsilon_decay=0.995)

    print("Training...")
    agent.learn(num_episodes=2000, max_steps=50, random_start=True, moving_goal=False)

    # Reset to original start position for testing
    env.set_start_position(start)
    env.reset()
    agent.epsilon = 0
    done = False
    steps = 0
    print("\nTesting learned policy...")

    plt.figure(figsize=(12, 6))
    plt.ion()

    while not done and steps < 50:
        env.render(pause_time=0.5, show_trajectory=True, show_policy=True, agent=agent)
        action = agent.act(env.agent.pos)
        _, _, done = env.step(action)
        steps += 1

    plt.ioff()
    plt.show()
    print("Done!")

